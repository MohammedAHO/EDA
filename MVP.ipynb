{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first call the packages that is going to be used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta, datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a funcion that will read the dates and return the data from the MTA site and save it as a data frame. The dates formated in order to be used in the next function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_MTA_data(start_ = date(2014, 10, 25), end_= datetime.date(datetime.now())): \n",
    "    df_url = pd.DataFrame()\n",
    "    for i in pd.date_range(start=start_, end=end_, freq='7D'):\n",
    "        ii = str(i)\n",
    "        formated_date = ii[2:4] + ii[5:7] + ii[8:10]\n",
    "        df_url = df_url.append(pd.read_csv(\"http://web.mta.info/developers/data/nyct/turnstile/turnstile_\"+formated_date+\".txt\"))\n",
    "    return df_url.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the starting date must be in the list shown in: http://web.mta.info/developers/turnstile.html.\n",
    "\n",
    "\n",
    "\n",
    "The starting date should also be on October 25, 2014, or after that but not before. The reason is that the data structure differs in files uploaded before that which will correspond to errors and nullities in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2016, 9, 3)\n",
    "end_date = date(2016, 12, 17)\n",
    "df = download_MTA_data(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a database, we fisrt downloaded the data as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'C:/Users/Windows10/SDAIA_Bootcamp/NBM_EDA_Gamma/EDA_MVP_Moh_Os.csv'\n",
    "# df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we created a local database using SQLite. Now, we import the data and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine = create_engine(\"sqlite:///eda_1.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_sql('SELECT * FROM df;', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of the date and time columns\n",
    "print(df[['DATE', 'TIME']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now want to fisrt create a 'date_time' column and convert the 'date' \n",
    "# column from object to  date type \n",
    "df['DATE_TIME'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'])\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['DATE_TIME']].dtypes)\n",
    "print(df[['DATE']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each compination of C/A, UNIT, SCP, and STATION represents a unique turnstile. Where:\n",
    "<br>C/A: Control Area.\n",
    "<br>UNIT: Remote Unit for a station.\n",
    "<br>SCP: Subunit Channel Position represents an specific address for a device.\n",
    "<br>STATION: Represents the station name the device is located at.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first clear the columns names so that we can use it.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we clean columns names and remove all unnecessary spaces.\n",
    "df.columns = [column.strip() for column in df.columns]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date is recorded every 4 hours, and the ENTRIES and EXITS columns are cumulative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To exploring the data, we will group by C/A, UNIT, SCP, STATION and DATE and get \n",
    "# entries and exits. This data represends the commulative ENTRIES and EXITS for each turnstile every\n",
    "# four hours.\n",
    "turnstile_df = df.groupby(['C/A', 'UNIT', 'SCP', 'STATION', 'DATE', 'DATE_TIME'],as_index=False)[['ENTRIES', 'EXITS']].min() \n",
    "turnstile_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the difference between the ENTRIES_PFH and the EXITS_PFH. This way, we can know the number of people inside a specific station in 4 hours duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, we need to clean the new columns and make sure data is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may face some cases where the ENTRIES or EXITS are negative, for these situations we will \n",
    "# deal with it using the following:\n",
    "\n",
    "# First, since we know each compination of C/A, UNIT, SCP, and STATION represents a unique turnstile.\n",
    "# We will force the value of entries equals to zero when it is subtracted from different turnstiles.\n",
    "turnstile_df['TURNSTILE_ID'] = turnstile_df[\"C/A\"] + turnstile_df[\"UNIT\"] + turnstile_df[\"SCP\"] + turnstile_df[\"STATION\"]\n",
    "\n",
    "turnstile_df['ENTRIES_PFH']=np.where(turnstile_df['TURNSTILE_ID'] == turnstile_df[\"TURNSTILE_ID\"].shift(-1), \n",
    "                                     turnstile_df['ENTRIES'].diff().shift(-1) ,\n",
    "                                     0)\n",
    "turnstile_df['EXITS_PFH']=np.where(turnstile_df['TURNSTILE_ID'] == turnstile_df[\"TURNSTILE_ID\"].shift(-1), \n",
    "                                   turnstile_df['EXITS'].diff().shift(-1),\n",
    "                                   0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are negative values\n",
    "turnstile_df[(turnstile_df.ENTRIES_PFH < 0) | (turnstile_df.EXITS_PFH < 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we deal with negative numbers\n",
    "turnstile_df.loc[turnstile_df.ENTRIES_PFH < 0,'ENTRIES_PFH'] = abs(turnstile_df[turnstile_df.ENTRIES_PFH < 0].ENTRIES_PFH)          \n",
    "turnstile_df.loc[turnstile_df.EXITS_PFH < 0,'EXITS_PFH'] = abs(turnstile_df[turnstile_df.EXITS_PFH < 0].EXITS_PFH)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are negative values\n",
    "turnstile_df[(turnstile_df.ENTRIES_PFH < 0) | (turnstile_df.EXITS_PFH < 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for outliers \n",
    "we will create a threshold of (14,400) that means it is impossible for a single turnstile to have more than one person per seconds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 14400\n",
    "turnstile_df = turnstile_df[(turnstile_df.ENTRIES_PFH < threshold)|(turnstile_df.EXITS_PFH < threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column representing the number of people inside counted by a turnstile\n",
    "turnstile_df['PEOPLE_INSIDE'] = turnstile_df['ENTRIES_PFH'] - turnstile_df['EXITS_PFH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us now group the turnstile dataframe by stations so that we see the daily entries of that station.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df = turnstile_df.groupby(by=['STATION', 'DATE'],as_index=False)[['ENTRIES_PFH', 'EXITS_PFH', 'PEOPLE_INSIDE']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take the absolute value of the data 'PEOPLE_INSIDE' column to know how many people were there in the station in the four hours period. The reason why we considered negative values in the 'PEOPLE_INSIDE' column is that we are looking for people inside the station, either arriving or going to get into the subway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df.loc[station_df.PEOPLE_INSIDE < 0,'PEOPLE_INSIDE'] = abs(station_df[station_df.PEOPLE_INSIDE < 0].PEOPLE_INSIDE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the boxplot of the'PEOPLE_INSIDE' columns for a station '1 AV'\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "sns.boxplot(station_df[(station_df['PEOPLE_INSIDE']<2600)&(station_df['STATION']=='1 AV')]['STATION'], station_df[(station_df['PEOPLE_INSIDE']<2600)&(station_df['STATION']=='1 AV')]['PEOPLE_INSIDE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us select a station '1 AV' and see the plot of the PEOPLE_INSIDE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(station_df[station_df['STATION'] == '1 AV']['DATE'], station_df[station_df['STATION'] == '1 AV']['PEOPLE_INSIDE'])\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Entries')\n",
    "plt.title('Station 1AV Daily Entries', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see it more closely, for few weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(station_df[(station_df['STATION'] == '1 AV')&\n",
    "                    (station_df['DATE'] > '2016-09-01')&\n",
    "                    (station_df['DATE'] < '2016-09-23')]['DATE'], \n",
    "         station_df[(station_df['STATION'] == '1 AV')&\n",
    "                    (station_df['DATE'] > '2016-09-01')&\n",
    "                    (station_df['DATE'] < '2016-09-23')]['PEOPLE_INSIDE'])\n",
    "plt.rc('xtick', labelsize=10) \n",
    "plt.rc('ytick', labelsize=10)\n",
    "plt.rc('axes', labelsize=15)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Entries')\n",
    "plt.title('Station 1AV Daily Entries', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see it more closely, for few weeks and for all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_df['DAYS'] = pd.to_datetime(station_df['DATE']).dt.dayofweek\n",
    "station_df['WEEK'] =  pd.to_datetime(station_df['DATE']).dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_plot = station_df.groupby([\"WEEK\",\"DAYS\"])[['PEOPLE_INSIDE']].sum()\n",
    "days_to_plot.index[[0,1,2][2]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,5));\n",
    "plt.xlabel(\"DAYS\")\n",
    "plt.ylabel(\"ENTRIES PER DAY\")\n",
    "#ignore week 34\n",
    "start_indx = days_to_plot.index[0]\n",
    "\n",
    "for i in range(15):\n",
    "    plt.plot(days_to_plot[days_to_plot['PEOPLE_INSIDE'] < 0.2e7].loc[35+i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the plot that the people inside column reaches is maximum (in most cases) in weekdays. So, we will focus on these days when giving the recomendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
